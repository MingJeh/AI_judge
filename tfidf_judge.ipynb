{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "572ed629",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn.preprocessing import normalize\n",
    "import jieba\n",
    "import re\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9471f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 dataframe 顯示內容大小\n",
    "# 顯示所有列\n",
    "pd.set_option('display.max_columns', None)\n",
    "# 顯示所有行\n",
    "pd.set_option('display.max_rows', None)\n",
    "# 設置 value 的顯示長度為 100，預設為 50\n",
    "pd.set_option('max_colwidth',1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641218e8",
   "metadata": {},
   "source": [
    "`將 main 和 content 合併，以計算 tf-idf 與 worcloud 用`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb503d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('D:/期末專題/AI法官/rawData/forModel.csv', encoding='utf-8')\n",
    "raw.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773d8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "m = raw.loc[:, 'judge_main']\n",
    "c = raw.loc[:, 'judge_content']\n",
    "m_c_lst = []  # main + content\n",
    "\n",
    "for i in range(0, len(m)):\n",
    "    m_c[i] = ''.join(m[i]+c[i])\n",
    "    m_c_lst.append(m_c[i])\n",
    "# m_c\n",
    "    \n",
    "\n",
    "raw.insert(4, 'judge_main_content', m_c_lst)\n",
    "\n",
    "raw.to_csv('D:/期末專題/AI法官/rawData/forModel_mainContent.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a821237",
   "metadata": {},
   "source": [
    "### 建立各罪 csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d158da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = [1, 2, 3, 4]\n",
    "ll = ['a', 'b', 'c', 'd']\n",
    "\n",
    "# a = list(zip(l, ll))\n",
    "# a_dict = dict(a)\n",
    "for i in l.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c06d7bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('D:/期末專題/AI法官/rawData/forModel_mainContent.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aeafec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "judge_multicrimeCategory\n",
    "\n",
    "1 傷害直系血親尊親屬致死\n",
    "2 傷害直系血親尊親屬\n",
    "3 傷害致死\n",
    "4 過失傷害致重傷\n",
    "5 傷害致重傷\n",
    "6 重傷害未遂\n",
    "7 重傷致死\n",
    "8 重傷害\n",
    "9 教唆傷害\n",
    "10 肇事逃逸\n",
    "11 過失傷害\n",
    "12 傷害\n",
    "13 無罪\n",
    "'''\n",
    "df_1 = raw[(raw['judge_multiCrimeCategory'] == '傷害直系血親尊親屬致死')]\n",
    "# print(len(df_1))\n",
    "df_2 = raw[(raw['judge_multiCrimeCategory'] == '傷害直系血親尊親屬')]\n",
    "df_3 = raw[(raw['judge_multiCrimeCategory'] == '傷害致死')]\n",
    "df_4 = raw[(raw['judge_multiCrimeCategory'] == '過失傷害致重傷')]\n",
    "df_5 = raw[(raw['judge_multiCrimeCategory'] == '傷害致重傷')]\n",
    "df_6 = raw[(raw['judge_multiCrimeCategory'] == '重傷害未遂')]\n",
    "df_7 = raw[(raw['judge_multiCrimeCategory'] == '重傷致死')]\n",
    "df_8 = raw[(raw['judge_multiCrimeCategory'] == '重傷害')]\n",
    "df_9 = raw[(raw['judge_multiCrimeCategory'] == '教唆傷害')]\n",
    "df_10 = raw[(raw['judge_multiCrimeCategory'] == '肇事逃逸')]\n",
    "df_11 = raw[(raw['judge_multiCrimeCategory'] == '過失傷害')]\n",
    "df_12 = raw[(raw['judge_multiCrimeCategory'] == '傷害')]\n",
    "df_13 = raw[(raw['judge_multiCrimeCategory'] == '無罪')]\n",
    "\n",
    "lst = [df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_12, df_13]\n",
    "crime = ['傷害直系血親尊親屬致死', \n",
    "         '傷害直系血親尊親屬', \n",
    "         '傷害致死', \n",
    "         '過失傷害致重傷', \n",
    "         '傷害致重傷', \n",
    "         '重傷害未遂', \n",
    "         '重傷致死', \n",
    "         '重傷害',\n",
    "         '教唆傷害',\n",
    "         '肇事逃逸',\n",
    "         '過失傷害',\n",
    "         '傷害',\n",
    "         '無罪']\n",
    "\n",
    "for i in range(1, 14):\n",
    "    lst[i-1].to_csv(f'D:/期末專題/AI法官/rawData/crimeCategory/crimeCategory_{i}_{crime[i-1]}.csv', encoding='utf-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac51ce6",
   "metadata": {},
   "source": [
    "### TF：依各罪比較"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d251fbea",
   "metadata": {},
   "source": [
    "* step 1 `讀取 該罪對應內文` \n",
    "\n",
    "* step 2 `將 內文+正規+結巴 存成 txt`\n",
    "\n",
    "* step 3 `計算 TF/IDF/TF-IDF 內文_正規_結巴.txt` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07ead45",
   "metadata": {},
   "source": [
    "`依各罪直接存成一個 corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d6f369f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Jeh\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.659 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "開始存取 傷害直系血親尊親屬致死 的 jieba.txt\n",
      "開始存取 傷害直系血親尊親屬 的 jieba.txt\n",
      "開始存取 傷害致死 的 jieba.txt\n",
      "開始存取 過失傷害致重傷 的 jieba.txt\n",
      "開始存取 傷害致重傷 的 jieba.txt\n",
      "開始存取 重傷害未遂 的 jieba.txt\n",
      "開始存取 重傷致死 的 jieba.txt\n",
      "開始存取 重傷害 的 jieba.txt\n",
      "開始存取 肇事逃逸 的 jieba.txt\n",
      "開始存取 過失傷害 的 jieba.txt\n",
      "開始存取 傷害 的 jieba.txt\n",
      "開始存取 無罪 的 jieba.txt\n",
      "總花費時間 : 452.15 分鐘\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "df1 傷害直系血親尊親屬致死\n",
    "df2 傷害直系血親尊親屬\n",
    "df3 傷害致死\n",
    "df4 過失傷害致重傷\n",
    "df5 傷害致重傷\n",
    "df6 重傷害未遂\n",
    "df7 重傷致死\n",
    "df8 重傷害\n",
    "df9 教唆傷害\n",
    "df10 肇事逃逸\n",
    "df11 過失傷害\n",
    "df12 傷害\n",
    "df13 無罪\n",
    "\n",
    "'''\n",
    "\n",
    "#lst = [df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10, df_11, df_12, df_13]\n",
    "\n",
    "crime = ['傷害直系血親尊親屬致死', \n",
    "         '傷害直系血親尊親屬', \n",
    "         '傷害致死', \n",
    "         '過失傷害致重傷', \n",
    "         '傷害致重傷', \n",
    "         '重傷害未遂', \n",
    "         '重傷致死', \n",
    "         '重傷害',\n",
    "         '教唆傷害',\n",
    "         '肇事逃逸',\n",
    "         '過失傷害',\n",
    "         '傷害',\n",
    "         '無罪']\n",
    "startTime = time.time()\n",
    "for i in range(0, len(crime)):\n",
    "    # step 1 讀取 該罪對應內文\n",
    "    df = pd.read_csv(f'D:/期末專題/AI法官/rawData/crimeCategory/crimeCategory_{i+1}_{crime[i]}.csv', encoding='utf-8')\n",
    "    df_content = df.loc[:, 'judge_main_content']\n",
    "\n",
    "    # step 2 將 內文+正規+結巴 存成 txt\n",
    "    with open(f'D:/期末專題/AI法官/rawData/crimeCategory/crimeCategory_{i+1}_{crime[i]}.txt', 'w', encoding='utf-8') as f:\n",
    "        contlst = []\n",
    "        for content in df_content:\n",
    "            contlst.append(content)\n",
    "        \n",
    "        data_content = ''.join(contlst)\n",
    "        #print(data_content)\n",
    "    \n",
    "        # re\n",
    "        contlst_re = []\n",
    "        r4 = \"\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\，。=？、：“”‘’￥……()《》【】]|[a-zA-Z]+|（|）|，|。|；|○|「|」\"\n",
    "        data_content_ = re.sub(r4,'',data_content)\n",
    "        contlst_tmp = list(data_content_)\n",
    "    \n",
    "        for content in contlst_tmp:\n",
    "            contlst_re.append(content)\n",
    "    \n",
    "        data_content_re = ''.join(contlst_re)\n",
    "\n",
    "        #print(data_content_re)\n",
    "    \n",
    "    \n",
    "        # jieba\n",
    "        # load 自訂義 dict\n",
    "        dict = 'C:/Users/Jeh/word2vec/dict_judge_update_new.txt'\n",
    "        jieba.load_userdict(dict)\n",
    "           \n",
    "        # load 自訂義 stopword\n",
    "        stopword = 'C:/Users/Jeh/word2vec/stopword_judge_update.txt'\n",
    "        stopwords = [line.strip() for line in open(stopword, 'r', encoding='utf-8').readlines()]\n",
    "        data_content_re_lst = str(data_content_re)\n",
    "        f.write(data_content_re_lst)\n",
    "\n",
    "    with open(f'D:/期末專題/AI法官/rawData/crimeCategory/crimeCategory_{i+1}_{crime[i]}_jeba.txt', 'w', encoding='utf-8') as new_f:\n",
    "        with open(f'D:/期末專題/AI法官/rawData/crimeCategory/crimeCategory_{i+1}_{crime[i]}.txt', 'r', encoding='utf-8') as f:            \n",
    "            for times, data in enumerate(f, 1):\n",
    "                print(f'開始存取 {crime[i]} 的 jieba.txt')\n",
    "                data = jieba.cut(data)\n",
    "                data = [word for word in data if word != ' ']\n",
    "                data = [word for word in data if word not in stopwords]  # 停用字\n",
    "                data = ' '.join(data)  # 將 list 轉成 string\n",
    "                new_f.write(data)\n",
    "\n",
    "print(f\"總花費時間 : {((time.time() - startTime)/60):.2f} 分鐘\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168cfcc0",
   "metadata": {},
   "source": [
    "### `依各罪的各篇判決書內文存成 n個 corpus`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1368c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 罪類別：其他類 -> 教唆傷害c1 (0筆) + 肇事逃逸c2  (6筆)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a20072",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "crime = ['傷害直系血親尊親屬致死', \n",
    "         '傷害直系血親尊親屬', \n",
    "         '傷害致死', \n",
    "         '過失傷害致重傷', \n",
    "         '傷害致重傷', \n",
    "         '重傷害未遂', \n",
    "         '重傷致死', \n",
    "         '重傷害',\n",
    "         '教唆傷害',\n",
    "         '肇事逃逸',\n",
    "         '過失傷害',\n",
    "         '傷害',\n",
    "         '無罪']\n",
    "\n",
    "startTime = time.time()\n",
    "for i in range(0, len(crime)):\n",
    "    folderPath = f'D:/期末專題/AI法官/rawData/crimeCategory/TF-IDF/{i+1}_{crime[i]}'\n",
    "    if not os.path.exists(folderPath):\n",
    "        os.makedirs(folderPath)\n",
    "    folderPath_jieba = f'D:/期末專題/AI法官/rawData/crimeCategory/TF-IDF/{i+1}_{crime[i]}/jieba'\n",
    "    if not os.path.exists(folderPath_jieba):\n",
    "        os.makedirs(folderPath_jieba)\n",
    "\n",
    "    # step 1 讀取 該罪對應內文\n",
    "    dfn = pd.read_csv(f'D:/期末專題/AI法官/rawData/crimeCategory/crimeCategory_{i+1}_{crime[i]}.csv', encoding='utf-8')\n",
    "    dfn_content = dfn.loc[:, 'judge_main_content']\n",
    "# print(len(dfn_content))  #5 63 14 53 34 15 2 303 0 6 44625 18816 2217\n",
    "        \n",
    "        \n",
    "    # step 2 將 內文+正規+結巴 存成 txt\n",
    "    # 將 每筆內文 存成 txt\n",
    "    for index in range(0, len(dfn_content)):\n",
    "        with open(f'{folderPath}/{index+1}_{crime[i]}.txt', 'w', encoding='utf-8') as f1:\n",
    "            contlst = []\n",
    "            for content in dfn_content[index]:\n",
    "                contlst.append(content)\n",
    "        \n",
    "            data_content = ''.join(contlst)\n",
    "#             print(len(data_content)\n",
    "    \n",
    "            # re\n",
    "            contlst_re = []\n",
    "            r4 = \"\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\，。=？、：“”‘’￥……()《》【】]|[a-zA-Z]+|（|）|，|。|；|○|「|」\"\n",
    "            data_content_ = re.sub(r4,'',data_content)\n",
    "            contlst_tmp = list(data_content_)\n",
    "    \n",
    "            for content in contlst_tmp:\n",
    "                contlst_re.append(content)\n",
    "    \n",
    "            data_content_re = ''.join(contlst_re)\n",
    "            #print(data_content_re)\n",
    "    \n",
    "            # jieba\n",
    "            # load 自訂義 dict\n",
    "            dict = 'C:/Users/Jeh/word2vec/dict_judge_update.txt'\n",
    "            jieba.load_userdict(dict)\n",
    "           \n",
    "            # load 自訂義 stopword\n",
    "            stopword = 'C:/Users/Jeh/word2vec/stopword_judge_update.txt'\n",
    "            stopwords = [line.strip() for line in open(stopword, 'r', encoding='utf-8').readlines()]\n",
    "    \n",
    "            data_content_re_lst = str(data_content_re)\n",
    "            f1.write(data_content_re_lst)\n",
    "    \n",
    "        with open(f'{folderPath_jieba}/{index+1}_jieba_{crime[i]}.txt', 'w', encoding='utf-8') as new_f:\n",
    "            with open(f'{folderPath}/{index+1}_{crime[i]}.txt', 'r', encoding='utf-8') as f:\n",
    "                for times, data in enumerate(f, 1):\n",
    "                    print(f'開始存取 {crime[i]} 的 jieba.txt')\n",
    "                    data = jieba.cut(data)\n",
    "                    data = [word for word in data if word != ' '] \n",
    "                    data = [word for word in data if word not in stopwords] # 停用字\n",
    "                    data = ' '.join(data)  # 將 list 轉成 string\n",
    "                    new_f.write(data)\n",
    "print(f\"總花費時間 : {((time.time() - startTime)/60):.2f} 分鐘\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46987f63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d58284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05529ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 3 計算各類罪的 TF/IDF/TF-IDF 內文_正規_結巴.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6d2d3",
   "metadata": {},
   "source": [
    "`TF`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803dc1f2",
   "metadata": {},
   "source": [
    "`存取各類罪的各內文檔案`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff29c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "crime = ['1_傷害直系血親尊親屬致死', \n",
    "         '2_傷害直系血親尊親屬', \n",
    "         '3_傷害致死', \n",
    "         '4_過失傷害致重傷', \n",
    "         '5_傷害致重傷', \n",
    "         '6_重傷害未遂', \n",
    "         '7_重傷致死', \n",
    "         '8_重傷害',\n",
    "         '9_教唆傷害',\n",
    "         '10_肇事逃逸',\n",
    "         '11_過失傷害',\n",
    "         '12_傷害',\n",
    "         '13_無罪']\n",
    "crimeName = crime[11]\n",
    "crime_words = []  # 存該罪全部的 word\n",
    "count = 0  # 記錄各罪的總篇數\n",
    "for filename in glob.glob(f'D:/期末專題/AI法官/rawData/crimeCategory/TF-IDF/{crimeName}/jieba/*.txt'):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r', encoding='utf-8') as f:\n",
    "        txtn = f.read()\n",
    "        txtn_s = txtn.split(' ')\n",
    "        for word in txtn_s:\n",
    "            crime_words.append(word)  # 存該罪全部的 word\n",
    "    count+=1\n",
    "\n",
    "bow_set = set(crime_words)  # 定義詞袋 bag of words\n",
    "# print(len(bow_set))\n",
    "wordDict = dict.fromkeys(bow_set, 0)  # 建立詞袋 dict，紀錄每個字的數量\n",
    "wordDict_record = dict.fromkeys(bow_set, 0)\n",
    "\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "countN = 1  # 記錄第幾篇\n",
    "df = pd.DataFrame([wordDict])\n",
    "for filename in glob.glob(f'D:/期末專題/AI法官/rawData/crimeCategory/TF-IDF/{crimeName}/jieba/*.txt'):\n",
    "    wordDict = dict.fromkeys(bow_set, 0)  # 建立詞袋 dict，紀錄每個字的數量\n",
    "    df_sum = pd.DataFrame([wordDict])\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r', encoding='utf-8') as f:\n",
    "        txtn = f.read()\n",
    "        txtn_s = txtn.split(' ')  # 存該罪各篇的 word  ->list\n",
    "        print('開始存取各篇內文')\n",
    "        print(f'第 {count} of {countN} 篇')\n",
    "        for word in txtn_s:\n",
    "            wordDict[word]+=1\n",
    "            wordDict_record[word]+=1\n",
    "        countN+=1\n",
    "        '''\n",
    "        tfx,y：frequency of x(word) in y(docs)\n",
    "        N：total number of y(docs)\n",
    "        dfx：number of y(docs) contains x(word)\n",
    "        '''\n",
    "#     print(len(txtn_s))\n",
    "#     print(txtn_s)\n",
    "\n",
    "    df = df.append(wordDict, ignore_index=True)  # 記錄各判決內容的 tf\n",
    "# print(df)\n",
    "df_sum = df_sum.append(wordDict_record, ignore_index=True)  # 紀錄全判決內容的 tf\n",
    "# print(df_sum)\n",
    "    \n",
    "df_ = df.drop([0,])  # 刪除 各判決內容的tf 第一個空值 row\n",
    "df_record = df_sum.drop([0,])  # 刪除 全判決內容的tf 第一個空值 row\n",
    "# print(df_record)\n",
    "df_final = df_.append(df_record, ignore_index=True)  # 各判決內容的 tf\n",
    "# print(df_final)\n",
    "n_lst = []\n",
    "for i in range(0, count+1):\n",
    "    n_lst.append(i)\n",
    "n_lst.sort(reverse=True)\n",
    "# print(n_lst)\n",
    "df_final_T = df_final.reindex(n_lst).T # 全判決內容的tf + 各判決內容的tf\n",
    "# print(df_final_T)\n",
    "df_final_T.to_csv(f'D:/期末專題/AI法官/rawData/crimeCategory/TF-IDF/{crimeName}.csv', encoding='utf-8-sig')\n",
    "\n",
    "print(f\"總花費時間 : {((time.time() - startTime)/60):.2f} 分鐘\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# tf-idf\n",
    "# tfidf = {}\n",
    "# for word, val in tfBow.items():\n",
    "#     tfidf[word] = val*idfs[word]\n",
    "# return tfidf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# tfDict = {}\n",
    "# bowCount = len(txtn_s)\n",
    "# for word, count in wordDict.items():\n",
    "#         tfDict[word] = count/float(bowCount)\n",
    "#     return tfDict\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cf06f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8531d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ab3699",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e4a469",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8302f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c7fbf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29be0984",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4c07b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6194d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21785e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dcfce33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d6a97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e5973e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d2287",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576cbc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a44d18d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7a19db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c91b2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e759296d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8cb3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e06c8cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc7e885",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690f85ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9352bbe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5aa413",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476ee673",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca64ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dca8c62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcbc0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1d5d12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7671170",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed476ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc911b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0249bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fe888",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6013d57",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce99e6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad4eeca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038123ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ca8684",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26357d07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22082009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27f2f3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5501ccb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c0b778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5190f973",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2ea55",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba1e111",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0610ded",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d19fd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601d6295",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475698e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fa98e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "502f4a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23812f0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061b4ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940d648d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fafb73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b804f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5219a9e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161eed1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c7591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5660a8c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb52f7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f524c473",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac5139c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c33b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f40cc1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0cf28e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f96a517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735b812",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973238be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15cf3f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36373144",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421b53a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f18cbe2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211e2eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17498c97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1457bb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9a0f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42de396c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7685b60a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8954e4e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5aa249",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71a18ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa9842",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a2034",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fed5615",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e970d39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061d1d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 測試 tf-idf\n",
    "\n",
    "import glob\n",
    "import os\n",
    "\n",
    "crime = ['1_傷害直系血親尊親屬致死', \n",
    "         '2_傷害直系血親尊親屬', \n",
    "         '3_傷害致死', \n",
    "         '4_過失傷害致重傷', \n",
    "         '5_傷害致重傷', \n",
    "         '6_重傷害未遂', \n",
    "         '7_重傷致死', \n",
    "         '8_重傷害',\n",
    "         '9_教唆傷害',\n",
    "         '10_肇事逃逸',\n",
    "         '11_過失傷害',\n",
    "         '12_傷害',\n",
    "         '13_無罪']\n",
    "crimeName = crime[0]\n",
    "crime_words = []  # 存該罪全部的 word\n",
    "count = 0  # 記錄各罪的篇數\n",
    "for filename in glob.glob(f'D:/期末專題/AI法官/rawData/crimeCategory/TF-IDF/{crimeName}/jieba/*.txt'):\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r', encoding='utf-8') as f:\n",
    "        txtn = f.read()\n",
    "        txtn_s = txtn.split(' ')\n",
    "        for word in txtn_s:\n",
    "            crime_words.append(word)  # 存該罪全部的 word\n",
    "    count+=1\n",
    "\n",
    "bow_set = set(crime_words)  # 定義詞袋 bag of words\n",
    "wordDict = dict.fromkeys(bow_set, 0)  # 建立詞袋 dict，紀錄每個字的數量\n",
    "# idfDict = dict.fromkeys(bow_set, 0)\n",
    "wordDict_record = dict.fromkeys(bow_set, 0)\n",
    "\n",
    "df = pd.DataFrame([wordDict])\n",
    "for filename in glob.glob(f'D:/期末專題/AI法官/rawData/crimeCategory/TF-IDF/{crimeName}/jieba/*.txt'):\n",
    "    wordDict = dict.fromkeys(bow_set, 0)  # 建立詞袋 dict，紀錄每個字的數量\n",
    "    idfDict = dict.fromkeys(bow_set, 0)\n",
    "    df_sum = pd.DataFrame([wordDict])\n",
    "    with open(os.path.join(os.getcwd(), filename), 'r', encoding='utf-8') as f:\n",
    "        txtn = f.read()\n",
    "        txtn_s = txtn.split(' ')  # 存該罪各篇的 word  ->list\n",
    "        for word in txtn_s:\n",
    "            wordDict[word]+=1\n",
    "            wordDict_record[word]+=1\n",
    "        \n",
    "         \n",
    "        '''\n",
    "        tfx,y：frequency of x(word) in y(docs)\n",
    "        N：total number of y(docs)\n",
    "        dfx：number of y(docs) contains x(word)\n",
    "        '''\n",
    "        # tf\n",
    "          \n",
    "        tfDict = {}  # -> tf frequency\n",
    "        idfDict = {}  # -> idf\n",
    "        N = count  # total number of y(docs)\n",
    "        bowCount = len(txtn_s)  # total number of y_content\n",
    "        for word, count_ in wordDict.items():\n",
    "            print(word)\n",
    "#             tfDict[word] = count_/float(bowCount)\n",
    "#         print(tfDict)\n",
    "        #print(\"=\"*200)\n",
    "        \n",
    "        # idf\n",
    "        idfDict = dict.fromkeys(tfDict)\n",
    "#         print(idfDict)\n",
    "        \n",
    "#         for word, val in idfDict.items():\n",
    "#             if val > 0:\n",
    "#                 idfDict[word] += 1\n",
    "#             idfDict[word] = math.log10(N/ float(val)) \n",
    "#         print(idfDict)\n",
    "        \n",
    "        \n",
    "\n",
    "#         for doc in wordDict:\n",
    "# #             print(doc)\n",
    "#             for word, val in doc.items():\n",
    "#                 if val > 0:\n",
    "#                     idfDict[word] += 1\n",
    "    \n",
    "        \n",
    "        # print(len(idfDict))  \n",
    "        \n",
    "\n",
    "'''\n",
    "    #print(len(txtn_s))\n",
    "    #print(txtn_s)\n",
    "\n",
    "#     df = df.append(wordDict, ignore_index=True)  # 記錄各判決內容的 tf\n",
    "#     df_sum = df_sum.append(wordDict_record, ignore_index=True)  # 紀錄全判決內容的 tf\n",
    "    \n",
    "# df_ = df.drop([0,])  # 刪除 各判決內容的tf 第一個空值 row\n",
    "# df_record = df_sum.drop([0,])  # 刪除 全判決內容的tf 第一個空值 row\n",
    "# df_final = df_.append(df_record, ignore_index=True)  # 各判決內容的 tf\n",
    "# n_lst = []\n",
    "# for i in range(1, count+1):\n",
    "#     n_lst.append(i)\n",
    "# n_lst.sort(reverse=True)\n",
    "# df_final_T = df_final.reindex(n_lst).T # 全判決內容的tf + 各判決內容的tf\n",
    "# print(df_final_T)\n",
    "#df_final_T.to_csv(f'D:/期末專題/AI法官/rawData/crimeCategory/TF-IDF/{crimeName}.csv', encoding='utf-8-sig')\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "# tf-idf\n",
    "# tfidf = {}\n",
    "# for word, val in tfBow.items():\n",
    "#     tfidf[word] = val*idfs[word]\n",
    "# return tfidf\n",
    "\n",
    "\n",
    "# tfDict = {}\n",
    "# bowCount = len(txtn_s)\n",
    "# for word, count in wordDict.items():\n",
    "#         tfDict[word] = count/float(bowCount)\n",
    "#     return tfDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80c47c9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3c5ea80",
   "metadata": {},
   "source": [
    "`test re`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890c91e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "te = '傷害致死，過失傷害致重傷後，重傷致無法行走，導致傷害致重傷'\n",
    "\n",
    "\n",
    "r4 = \"\\【.*?】+|\\《.*?》+|\\#.*?#+|[.!/_,$&%^*()<>+\"\"'?@|:~{}#]+|[——！\\，。=？、：“”‘’￥……()《》【】]|[a-zA-Z]+|（|）|，|。|；|○|「|」\"\n",
    "data_content = re.sub(r4,'',te)\n",
    "# print(data_content)\n",
    "content1 = list(data_content)\n",
    "\n",
    "with open('test_re.txt', 'w', encoding='utf-8') as new_f:\n",
    "    for a in content1:\n",
    "        content2 = ''.join(a)\n",
    "        new_f.write(content2)\n",
    "        \n",
    "with open('test_re_cut.txt', 'w', encoding='utf-8-sig') as new_f:\n",
    "     with open('test_re.txt', 'r', encoding='utf-8') as f:\n",
    "        # load 自訂義 dict\n",
    "        dict = './test.txt'\n",
    "        jieba.load_userdict(dict)\n",
    "           \n",
    "        # load 自訂義 stopword\n",
    "        # stopword = './stopword_judge.txt'\n",
    "        # stopwords = [line.strip() for line in open(stopword, 'r', encoding='utf-8').readlines()]\n",
    " \n",
    "    \n",
    "        for times, data in enumerate(f, 1):\n",
    "            print('data num:', times)\n",
    "            data = jieba.cut(data)\n",
    "            data = [word for word in data if word != ' ']\n",
    "            data = ' '.join(data)  # 將 list 轉成 string\n",
    "#             print(data)\n",
    "            new_f.write(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f328835",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
