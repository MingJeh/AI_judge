{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e019b72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora import WikiCorpus\n",
    "# wiki corpus\n",
    "\n",
    "\n",
    "wiki_corpus = WikiCorpus('zhwiki-20221120-pages-articles-multistream.xml.bz2', dictionary={})\n",
    "text_num = 0\n",
    "\n",
    "with open('wiki_text.txt', 'w', encoding='utf-8') as f:\n",
    "    for text in wiki_corpus.get_texts():\n",
    "        f.write(' '.join(text)+'\\n')\n",
    "        text_num += 1\n",
    "        if text_num % 10000 == 0:\n",
    "            print('{} articles processed.'.format(text_num))\n",
    "\n",
    "    print('{} articles processed.'.format(text_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682c59ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import time\n",
    "from opencc import OpenCC  # 將字體轉換\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "judge = open('./corpus_judge_ten_comma.txt', encoding='utf-8')\n",
    "wiki = open('./wiki_text.txt', encoding='utf-8')\n",
    "count_wiki = 1\n",
    "count_judge = 1\n",
    "with open('corpus_wiki_judge.txt', 'w', encoding='utf-8') as new_f:\n",
    "    for content_wiki in wiki:\n",
    "        new_f.write(''.join(content_wiki))\n",
    "        print('wiki :', count_wiki)\n",
    "        count_wiki+=1\n",
    "    for content_judge in judge:\n",
    "        new_f.write(''.join(content_judge))\n",
    "        print('judge :', count_judge)\n",
    "        count_judge+=1\n",
    "\n",
    "print(f\"總花費時間 : {((time.time() - startTime)/60):.2f} 分鐘\")       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c4de0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import time\n",
    "from opencc import OpenCC  # 將字體轉換\n",
    "# split word\n",
    "\n",
    "\n",
    "# Initial\n",
    "cc = OpenCC('s2t')  # 將簡體轉為繁體\n",
    "\n",
    "# wiki_text_seg.txt 已斷詞\n",
    "# wiki_text.txt 未斷詞\n",
    "\n",
    "# Tokenize\n",
    "startTime = time.time()\n",
    "with open('./corpus_wiki_judge_comma_jieba.txt', 'w', encoding='utf-8') as new_f:\n",
    "    with open('./corpus_wiki_judge.txt', 'r', encoding='utf-8') as f:\n",
    "        # load 自訂義 dict\n",
    "        dict = './dict_judge.txt'\n",
    "        jieba.load_userdict(dict)\n",
    "        \n",
    "        for times, data in enumerate(f, 1):\n",
    "            print('data num:', times)\n",
    "            data = cc.convert(data)\n",
    "            data = jieba.cut(data)\n",
    "            data = [word for word in data if word != ' ']\n",
    "            data = ' '.join(data)  # 將 list 轉成 string\n",
    "\n",
    "            new_f.write(data)\n",
    "            \n",
    "print(f\"總花費時間 : {((time.time() - startTime)/60):.2f} 分鐘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8cbeb78e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "總花費時間 : 46.20 分鐘\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "import time\n",
    "# train model\n",
    "\n",
    "# Settings\n",
    "seed = 666  # 亂數種子\n",
    "sg = 0  # sg = 0 ( CBOW ) ; sg = 1 ( Skip-gram ) \n",
    "window_size = 10  # 週圍的詞要看多少範圍\n",
    "vector_size = 250  # 轉成向量的維度\n",
    "min_count = 1  # 詞頻少於 min_count 的詞不會參與訓練\n",
    "workers = 12  # 訓練的並行數量，cpu\n",
    "epochs = 8  # 訓練的迭代次數 iter\n",
    "batch_words = 1000  # 每次給予多少詞量訓練\n",
    "\n",
    "startTime = time.time()\n",
    "train_data = word2vec.LineSentence('corpus_wiki_judge_comma_jieba.txt')\n",
    "model = word2vec.Word2Vec(\n",
    "    train_data,\n",
    "    min_count=min_count,\n",
    "    vector_size=vector_size,\n",
    "    workers=workers,\n",
    "    epochs=epochs,\n",
    "    window=window_size,\n",
    "    sg=sg,\n",
    "    seed=seed,\n",
    "    batch_words=batch_words\n",
    ")\n",
    "\n",
    "# save model\n",
    "model.save('model_wiki_judge_d250_sg_s10_ep8.model')\n",
    "\n",
    "print(f\"總花費時間 : {((time.time() - startTime)/60):.2f} 分鐘\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aabce0a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(250,)\n",
      "('業務過失傷害', 0.8761041164398193)\n",
      "('犯行', 0.7959913611412048)\n",
      "('過失傷害致人重傷', 0.7841423749923706)\n",
      "('業務過失傷害致人重傷', 0.7327824234962463)\n",
      "('傷犯行', 0.7051565647125244)\n",
      "('失重', 0.7017012238502502)\n",
      "('過失致人重傷害', 0.7005434036254883)\n",
      "('致重傷', 0.6871241927146912)\n",
      "('有業務過', 0.6834397315979004)\n",
      "('茶致', 0.6792193055152893)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.57760245"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "# test model\n",
    "\n",
    "# load model\n",
    "model = word2vec.Word2Vec.load('model_wiki_judge_d250_sg_s10_ep10')\n",
    "\n",
    "\n",
    "\n",
    "# load model ( gensim )\n",
    "# model = gensim.models.Word2Vec.load('word2vec.model')\n",
    "\n",
    "print(model.wv['過失傷害'].shape)\n",
    "\n",
    "for item in model.wv.most_similar('過失傷害'):\n",
    "    print(item)\n",
    "\n",
    "# 查看詞向量\n",
    "model.wv['過失傷害']\n",
    "\n",
    "\n",
    "# 查看最相近詞 positive / 最相近反義詞 negative  \n",
    "# model.wv.most_similar(positive=['英國', '倫敦'], negative=['法國'], topn=20)\n",
    "\n",
    "\n",
    "# 計算相似程度\n",
    "model.wv.similarity('過失傷害', '車禍')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1112abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "wiki_text_seg = ('./wiki_text_seg.txt')\n",
    "# Take word vectors\n",
    "word_vectors = np.array([model[w] for w in wiki_text_seg])\n",
    "\n",
    "# PCA, take the first 2 principal components\n",
    "twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "\n",
    "# Draw\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "for word, (x,y) in zip(words, twodim):\n",
    "    plt.text(x+0.05, y+0.05, word)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2e6a1a",
   "metadata": {},
   "source": [
    "### ============================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3906fe26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "from sklearn.decomposition import PCA\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "\n",
    "def display_pca_scatterplot(model, words):\n",
    "    # Take word vectors\n",
    "    word_vectors = np.array([model[w] for w in words])\n",
    "\n",
    "    # PCA, take the first 2 principal components\n",
    "    twodim = PCA().fit_transform(word_vectors)[:,:2]\n",
    "\n",
    "    # Draw\n",
    "    plt.figure(figsize=(6,6))\n",
    "    plt.scatter(twodim[:,0], twodim[:,1], edgecolors='k', c='r')\n",
    "    for word, (x,y) in zip(words, twodim):\n",
    "        plt.text(x+0.05, y+0.05, word)\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Download pre-trained GloVe embeddings, turn into Word2Vec format\n",
    "    glove_file = './data/glove.6B.100d.txt'\n",
    "    word2vec_glove_file = './data/glove.6B.100d.word2vec.txt'\n",
    "    if not os.path.isfile(word2vec_glove_file):\n",
    "        glove2word2vec(glove_file, word2vec_glove_file)\n",
    "\n",
    "    # Load model\n",
    "    model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
    "\n",
    "    # Take word, return most similar words\n",
    "    word = input('Similarity - Input word: ')\n",
    "    while word != '':\n",
    "        print(model.most_similar(word))\n",
    "        word = input('Similarity - Input word: ')\n",
    "\n",
    "    # Take a, b, c, return d for a : b = c : d\n",
    "    a = input('Analogy a : b = c : d - Input a: ')\n",
    "    b = input('Analogy a : b = c : d - Input b: ')\n",
    "    c = input('Analogy a : b = c : d - Input c: ')\n",
    "    while a != '' and b != '' and c != '':\n",
    "        print(model.most_similar(positive=[c, b], negative=[a]))\n",
    "        a = input('Analogy a : b = c : d - Input a: ')\n",
    "        b = input('Analogy a : b = c : d - Input b: ')\n",
    "        c = input('Analogy a : b = c : d - Input c: ')\n",
    "\n",
    "    # Display scatter plot for words\n",
    "    choice = input('2D word vector visualization - Input \\'default\\' for a default list of words, or a comma-separated list of words: ')\n",
    "    while choice != '':\n",
    "        if choice == 'default':\n",
    "            words = ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
    "                     'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "                     'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "                     'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "                     'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
    "                     'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "                     'school', 'college', 'university', 'institute']\n",
    "            display_pca_scatterplot(model, words=words)\n",
    "        else:\n",
    "            display_pca_scatterplot(model, words=[word.strip() for word in choice.split(',')])\n",
    "        choice = input('2D word vector visualization - Input \\'default\\' for a default list of words, or a comma-separated list of words: ')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc50a254",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['coffee', 'tea', 'beer', 'wine', 'brandy', 'rum', 'champagne', 'water',\n",
    "         'spaghetti', 'borscht', 'hamburger', 'pizza', 'falafel', 'sushi', 'meatballs',\n",
    "         'dog', 'horse', 'cat', 'monkey', 'parrot', 'koala', 'lizard',\n",
    "         'frog', 'toad', 'monkey', 'ape', 'kangaroo', 'wombat', 'wolf',\n",
    "         'france', 'germany', 'hungary', 'luxembourg', 'australia', 'fiji', 'china',\n",
    "         'homework', 'assignment', 'problem', 'exam', 'test', 'class',\n",
    "         'school', 'college', 'university', 'institute']\n",
    "display_pca_scatterplot(model, words)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
